import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import random
import math




# GENERATE DATA for AR(1)

T = 1000
x = np.zeros(T)

sigma2 = 1
alpha = 0.2
beta = 0.6
tau2 = sigma2/(1-beta)
eta = alpha/(1-beta)

for t in range (1,T,1):  
    x[t] = alpha + beta*x[t-1] + np.random.normal(0, sigma2)

fig = plt.figure(figsize=(6,4))
ax = fig.add_subplot(1,1,1)
ax.hist(x,bins=35 ,)
ax.set_xlabel("Value")
ax.set_ylabel("Frequency")
ax.set_title("Generated Data")
mu_obs=x.mean()
mu_obs


# SET LOG-LIKELIHOOD
def evaluateLogLikelihood(params):
    sigma2 = params[0]
    alpha = params[1]
    beta = params[2]
    
    return np.sum((1/np.sqrt(2*np.pi*sigma2)) * np.log(-(1/(2*sigma2))*(x[t] - alpha - beta*x[t-1])^2))
	
	
# MAIN STUFF

# initial guess for our parameters
guess = [1.0, 1.0, 1.0]
# Prepare storing MCMC chain as array of arrays
A = [guess]
# define stepsize of MCMC
stepsizes = [0.005, 0.005, 0.005]  # array of stepsizes
accepted  = 0.0

# Metropolis-Hastings with 10,000 iterations
for n in range(10000):
    old_alpha  = A[len(A)-1]  # old parameters value as array (the guess)
    old_loglik = evaluateLogLikelihood(old_alpha)
    
    # Suggest new candidate from Gaussian proposal distribution(Uniform) #Should be using probably a normal distribution or Jeffrey Prior
    new_alpha = np.zeros([len(old_alpha)])
    for i in range(len(old_alpha)):
    
        # Use stepsize provided for every dimension
        new_alpha[i] = random.gauss(old_alpha[i], stepsizes[i])
    new_loglik = evaluateLogLikelihood(new_alpha)
    
    # Accept new candidate
    if (new_loglik > old_loglik):
        A.append(new_alpha)
        accepted = accepted + 1.0  # monitor acceptance
    else:
        u = random.uniform(0.0,1.0)
        if (u < math.exp(new_loglik - old_loglik)):
            A.append(new_alpha)
            accepted = accepted + 1.0  # monitor acceptance
        else:
            A.append(old_alpha)

print("Acceptance rate = " + str(accepted/10000.0))

# RESULTS
# Discard first half of MCMC chain and thin out the rest.(BURN IN)
sigma2_estimate = []
alpha_estimate = []
beta_estimate = []

for n in range(5000,10000):
    if (n % 10 == 0):
        sigma2_estimate.append(A[n][0])
        alpha_estimate.append(A[n][1])
        beta_estimate.append(A[n][2])


# Print Monte-Carlo estimate of parameters
print("sigma2 mean:  "+str(np.mean(sigma2_estimate)))
print("sigma2 std: "+str(np.std(sigma2_estimate)))

print("alpha mean:  "+str(np.mean(alpha_estimate)))
print("alpha std: "+str(np.std(alpha_estimate)))

print("beta mean:  "+str(np.mean(beta_estimate)))
print("beta std: "+str(np.std(beta_estimate)))

plt.hist(sigma2_estimate, 20, lw=3)
plt.hist(alpha_estimate, 20, lw=3)
plt.hist(beta_estimate, 20, lw=3)

# For some reason it is still kind of wonky, I suspect that using a uniform prior probably has made it deviate from the ground truth.
